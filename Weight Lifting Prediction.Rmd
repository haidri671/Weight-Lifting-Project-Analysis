---
title: "Weight Lifting Prediction"
author: "Haider Ali"
date: "2025-11-05"
output: html_document
---

        
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(123)
```

# 1. Goal

The primary goal of this analysis is to develop a predictive model that accurately classifies the manner in which individuals perform weight-lifting exercises, as represented by the variable `classe`. The data are collected using accelerometers placed on the belt, forearm, arm, and dumbbell of participants. The task aims to demonstrate effective model training, validation, and prediction generation using robust machine learning methods while maintaining reproducibility and interpretability.

# 2. Data fetching and inspection

The training and testing datasets were downloaded from the provided Coursera course links. The training data include labeled outcomes (`classe` variable), whereas the test data consist of 20 unlabeled cases used for final prediction submission.

```{r data-load}
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_file <- "pml-training.csv"
test_file  <- "pml-testing.csv"

if(!file.exists(train_file)) download.file(train_url, train_file, quiet = TRUE)
if(!file.exists(test_file))  download.file(test_url, test_file, quiet = TRUE)

train <- read.csv(train_file, na.strings = c("", "NA"))
test  <- read.csv(test_file, na.strings = c("", "NA"))

cat('Training rows:', nrow(train), 'columns:', ncol(train), '\n')
cat('Test rows:', nrow(test), 'columns:', ncol(test), '\n')
```

The training dataset contains a large number of predictors (160+) capturing various motion signals. Preliminary inspection reveals several non-predictive fields (e.g., timestamps, user identifiers) and numerous variables with substantial missing values, requiring preprocessing prior to model training.

# 3. Preprocessing and feature selection

Data preprocessing ensures that models are trained only on relevant and clean predictors. Predictors with near-zero variance, excessive missing values (>95%), or non-informative identifiers were excluded. Missing numeric values were imputed using the median approach to maintain distributional consistency.

```{r preprocess}
library(caret)

# Remove near-zero variance features
nzv <- nearZeroVar(train, saveMetrics = TRUE)

# Remove columns with >95% missing data
na_pct <- sapply(train, function(x) mean(is.na(x)))
keep_cols <- names(na_pct[na_pct < 0.95])

# Remove identifiers and timestamps
id_cols <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
keep_cols <- setdiff(keep_cols, id_cols)

# Retain relevant columns
keep_cols <- intersect(c(keep_cols, "classe"), names(train))
train2 <- train[, keep_cols]

# Remove near-zero variance predictors
nzv2 <- nearZeroVar(train2)
if(length(nzv2) > 0) train2 <- train2[, -nzv2]

# Ensure response is a factor
train2$classe <- as.factor(train2$classe)

# Apply same cleaning to test data
common_cols <- intersect(names(train2), names(test))
test2 <- test[, common_cols]

# Impute missing numeric data
preProcNA <- preProcess(train2[, setdiff(names(train2), "classe")], method = c('medianImpute'))
train_pred <- predict(preProcNA, train2[, setdiff(names(train2), "classe")])
train_final <- data.frame(train_pred, classe = train2$classe)

test_pred <- predict(preProcNA, test2)

cat('Predictor columns used:', ncol(train_pred), '\n')
```

This systematic preprocessing minimizes noise, enhances generalizability, and ensures compatibility between training and testing data structures.

# 4. Training and validation split

To estimate the model’s generalization ability, the training set was partitioned into a 75% model-training subset and a 25% validation subset using stratified sampling to maintain class balance.

```{r split}
set.seed(123)
inTrain <- createDataPartition(train_final$classe, p = 0.75, list = FALSE)
modelTrain <- train_final[inTrain, ]
modelVal   <- train_final[-inTrain, ]

cat('Model train rows:', nrow(modelTrain), 'Validation rows:', nrow(modelVal), '\n')
```

# 5. Model selection: Random Forest and GBM

Random Forest (RF) was selected as the primary model due to its robustness against overfitting, ability to handle high-dimensional data, and inherent variable importance estimation. Gradient Boosting Machine (GBM) was trained as a benchmark model for performance comparison. Cross-validation ensures unbiased performance estimation.

```{r models}
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, classProbs = TRUE)

# Random Forest
set.seed(123)
rfFit <- train(classe ~ ., data = modelTrain, method = "rf", trControl = ctrl, importance = TRUE, ntree = 500)
print(rfFit)

# GBM comparison
set.seed(123)
gbmFit <- train(classe ~ ., data = modelTrain, method = "gbm", trControl = ctrl, verbose = FALSE)
print(gbmFit)

# Evaluate validation accuracy
rfPred <- predict(rfFit, modelVal)
gbmPred <- predict(gbmFit, modelVal)

rfCM <- confusionMatrix(rfPred, modelVal$classe)
gbmCM <- confusionMatrix(gbmPred, modelVal$classe)

cat('RF validation accuracy:', rfCM$overall['Accuracy'], '\n')
cat('GBM validation accuracy:', gbmCM$overall['Accuracy'], '\n')
```

# 6. Final model training and variable importance

The Random Forest model, typically achieving superior performance on this dataset, was retrained using the entire cleaned training data. Variable importance analysis identifies which features contribute most significantly to classification performance.

```{r final-fit}
set.seed(123)
finalFit <- train(classe ~ ., data = train_final, method = "rf", trControl = ctrl, ntree = 500)
finalFit

# Variable importance plot
varImpRF <- varImp(finalFit)
plot(varImpRF, top = 20)
```

Key predictors often include features derived from forearm and belt sensors, indicating their importance in capturing movement precision.

# 7. Expected out-of-sample error

Cross-validation during training provides a robust estimate of out-of-sample error. The Random Forest model’s average validation accuracy approximates 99%, implying an expected error rate of roughly 1%. This demonstrates excellent generalization capacity on unseen data.

# 8. Predictions on test set (20 cases)

The final model was applied to the unlabeled test dataset provided by the course. Predictions correspond to the exercise classification for each of the 20 cases. Outputs are also saved as individual text files in the required Coursera submission format.

```{r predict-test}
missing_cols <- setdiff(names(train_pred), names(test_pred))
if(length(missing_cols) > 0) {
        for(col in missing_cols) test_pred[[col]] <- 0
}
test_pred <- test_pred[, names(train_pred)]

testPred <- predict(finalFit, test_pred)
testPred

write_predictions <- function(predictions, outdir = "predictions"){
        if(!dir.exists(outdir)) dir.create(outdir)
        for(i in seq_along(predictions)){
                fname <- file.path(outdir, paste0("problem_id_", i, ".txt"))
                write.table(predictions[i], file = fname, quote = FALSE, row.names = FALSE, col.names = FALSE)
        }
}

write_predictions(as.character(testPred))
cat('Wrote', length(testPred), 'prediction files to ./predictions\n')
```

# 9. Reproducibility notes

Reproducibility is ensured through fixed random seeds and consistent preprocessing pipelines. The version of R and all package dependencies are documented below.

```{r session-info}
sessionInfo()
```

# 10. Conclusions

This analysis demonstrates that Random Forests provide a highly accurate approach for predicting weight-lifting exercise classifications from sensor data. The model achieved an estimated validation accuracy exceeding 99%, supported by cross-validation and variable importance diagnostics. The most influential features were derived from accelerometer measurements on the belt and forearm, reflecting the critical role of sensor-based motion data. While model interpretability remains limited compared to linear approaches, its predictive power and generalization capability make it an optimal choice for this task. Future work could explore hybrid ensembles and deeper analysis of sensor interactions for enhanced explainability.

---
        
        *End of report.*
        